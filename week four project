library(dplyr)
library(ggplot2)
library(lubridate)
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
library(corrplot)

#Data Loading
  TeData<- read.csv("pml-testing.csv", na.strings = c("NA", "#DIV/0!", ""))
  TrainingData<- read.csv("pml-training.csv", na.strings = c("NA", "#DIV/0!", ""))
#Data Transforamation
  TrainingData$cvtd_timestamp<- as.Date(TrainingData$cvtd_timestamp, format = "%m/%d/%Y %H:%M")
  TrainingData$Day<-factor(weekdays(TrainingData$cvtd_timestamp)) #Add day variable
#Data Cleaning
  #### Remove columns with NA missing values
    TrainingData <- TrainingData[, colSums(is.na(TrainingData)) == 0]
    TeData <- TeData[, colSums(is.na(TeData)) == 0] 
  #### Remove columns that are not relevant to accelerometer measurements.
    classe<- TrainingData$classe
    TrRemove<- grepl("^X|timestamp|window", names(TrainingData))
    TrainingData<- TrainingData[, !TrRemove]
    TrClean<- TrainingData[, sapply(TrainingData, is.numeric)]
    TrClean$classe<- classe
    TeRemove<- grepl("^X|timestamp|window", names(TeData))
    TeData<- TeData[, !TeRemove]
    TeClean<- TeData[, sapply(TeData, is.numeric)]
#Create Train and Test data sets
  set.seed(22519)
  inTrain <- createDataPartition(TrClean$classe, p=0.70, list=F)
  FintrainData <- TrClean[inTrain, ]
  FintestData <- TrClean[-inTrain, ]
#Data Modelling
  controlRf <- trainControl(method="cv", 5)
  rfmod<- train(classe ~., data=FintrainData, method="rf", trControl=controlRf, importance=TRUE, ntree=100)
  rfmod
#Accuacy of the model on Validation data set
  predictRfmod<- predict(rfmod, FintestData)
  confusionMatrix(FintestData$classe, predictRfmod)
  accuracy <- postResample(predictRfmod, FintestData$classe)
  accuracy
  Error <- 1 - as.numeric(confusionMatrix(FintestData$classe, predictRfmod)$overall[1])
  Error
  result <- predict(rfmod, TeClean[, -length(names(TeClean))])
  result
  \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
///////////////////////////////////////////////////////////output////////////////////////////////////////////////////////////////////
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
> library(dplyr)
> library(ggplot2)
> library(lubridate)
> library(caret)
> library(randomForest)
> library(rpart)
> library(rpart.plot)
> library(corrplot)
> 
> #Data Loading
>   TeData<- read.csv("pml-testing.csv", na.strings = c("NA", "#DIV/0!", ""))
>   TrainingData<- read.csv("pml-training.csv", na.strings = c("NA", "#DIV/0!", ""))
> #Data Transforamation
>   TrainingData$cvtd_timestamp<- as.Date(TrainingData$cvtd_timestamp, format = "%m/%d/%Y %H:%M")
>   TrainingData$Day<-factor(weekdays(TrainingData$cvtd_timestamp)) #Add day variable
> #Data Cleaning
>   #### Remove columns with NA missing values
>     TrainingData <- TrainingData[, colSums(is.na(TrainingData)) == 0]
>     TeData <- TeData[, colSums(is.na(TeData)) == 0] 
>   #### Remove columns that are not relevant to accelerometer measurements.
>     classe<- TrainingData$classe
>     TrRemove<- grepl("^X|timestamp|window", names(TrainingData))
>     TrainingData<- TrainingData[, !TrRemove]
>     TrClean<- TrainingData[, sapply(TrainingData, is.numeric)]
>     TrClean$classe<- classe
>     TeRemove<- grepl("^X|timestamp|window", names(TeData))
>     TeData<- TeData[, !TeRemove]
>     TeClean<- TeData[, sapply(TeData, is.numeric)]
> #Create Train and Test data sets
>   set.seed(22519)
>   inTrain <- createDataPartition(TrClean$classe, p=0.70, list=F)
>   FintrainData <- TrClean[inTrain, ]
>   FintestData <- TrClean[-inTrain, ]
> #Data Modelling
>   controlRf <- trainControl(method="cv", 5)
>   rfmod<- train(classe ~., data=FintrainData, method="rf", trControl=controlRf, importance=TRUE, ntree=100)
>   rfmod
Random Forest 

13737 samples
   52 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 10989, 10991, 10988, 10989, 10991 
Resampling results across tuning parameters:

  mtry  Accuracy   Kappa    
   2    0.9902446  0.9876590
  27    0.9911181  0.9887647
  52    0.9852942  0.9813962

Accuracy was used to select the optimal model using the largest value.
The final value used for the model was mtry = 27.
> #Accuacy of the model on Validation data set
>   predictRfmod<- predict(rfmod, FintestData)
>   confusionMatrix(FintestData$classe, predictRfmod)
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1673    0    0    0    1
         B    7 1128    4    0    0
         C    0    0 1021    5    0
         D    0    0   13  950    1
         E    0    0    1    7 1074

Overall Statistics
                                         
               Accuracy : 0.9934         
                 95% CI : (0.991, 0.9953)
    No Information Rate : 0.2855         
    P-Value [Acc > NIR] : < 2.2e-16      
                                         
                  Kappa : 0.9916         
 Mcnemar's Test P-Value : NA             

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9958   1.0000   0.9827   0.9875   0.9981
Specificity            0.9998   0.9977   0.9990   0.9972   0.9983
Pos Pred Value         0.9994   0.9903   0.9951   0.9855   0.9926
Neg Pred Value         0.9983   1.0000   0.9963   0.9976   0.9996
Prevalence             0.2855   0.1917   0.1766   0.1635   0.1828
Detection Rate         0.2843   0.1917   0.1735   0.1614   0.1825
Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839
Balanced Accuracy      0.9978   0.9988   0.9908   0.9923   0.9982
>   accuracy <- postResample(predictRfmod, FintestData$classe)
>   accuracy
Accuracy    Kappa 
0.993373 0.991617 
>   Error <- 1 - as.numeric(confusionMatrix(FintestData$classe, predictRfmod)$overall[1])
>   Error
[1] 0.006627018
>   result <- predict(rfmod, TeClean[, -length(names(TeClean))])
>   result
 [1] B A B A A E D B A A B C B A E E A B B B
Levels: A B C D E
